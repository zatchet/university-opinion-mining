subreddit_guess_benchmark.csv:

This is an LLM-generated dataset (we used an existing dataset of university names and their subreddits) that tests whether the model can correctly output the name of the subreddit for the given question. We will fine-tune a model to be really good at this task. An untuned LLM is ok at some of the questions in the benchmark (it gets it right less than 50% of the time), but it will need to be better. When testing, we would deem the model successful if it gets the answer correct 80% of the time. 

final_llm_benchmark.csv:

This can be thought of as the end-to-end test for our system. The question is the input, and the sentiment (positive, neutral, negative) is the output. An untuned LLM with no RAG done previously will obviously not be very good at this. When testing, our success threshold will be approximately 40-50% as this would be better than guessing at random (which would be ~33%). 